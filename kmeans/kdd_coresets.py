# -*- coding: utf-8 -*-
"""KDD_sklearn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gKlTAFnFPN4IJsO0_pxcb_j4XzzBS4Xo
"""

import numpy as np
import pandas as pd
from numpy import linalg as LA
import matplotlib.pyplot as plt

"""Real Dataset"""

traindata = pd.DataFrame(pd.read_csv("../kdd/bio_train.dat", '\t'))
dataset = np.array(traindata)
print(dataset.shape)
"""k-Means generalized

"""

def nearest(pt,Q):
  min_dist_sq = 10**18
  closest_center = 0
  for c in Q :
    c = np.array(c)
    pt = np.array(pt)
    dist_sq = (LA.norm(c-pt))**2
    # print(dist_sq)
    if dist_sq < min_dist_sq : 
      min_dist_sq = dist_sq
      closest_center = c
  # print(closest_center)
  return closest_center, min_dist_sq

def kmeans_plusplus(dataset,k):
  Q = {}
  #choose the first center randomly
  index = np.random.randint(0,len(dataset))
  Q[tuple(dataset[index])] = 0 
  
  for i in range(k-1):
    p = []
    d = []
    sum = 0
    # print(Q)
    for pt in dataset:
      # print(pt)
      if tuple(pt) in Q:
        d.append(0)
        continue

      closest_center , min_dist_sq = nearest(pt,Q)
      d.append(min_dist_sq)
      sum += min_dist_sq
    
    for j in range(len(dataset)):
      p.append(d[j]/sum)
    

    next_center_index = np.random.choice([i for i in range(len(dataset))],p = p)
    Q[tuple(dataset[next_center_index])] = 0
  

  return Q

def kmeans(Q,dataset,wt):

    for itr in range(10):
      # print(Q)
      assignment = {}
      for c in Q :
        assignment[c] = []
      #assign each point to nearest center
      for pt in dataset :
        if tuple(pt) in Q :
          continue

        closest_center , min_dist_sq = nearest(pt,Q)
        # print(closest_center)
        assignment[tuple(closest_center)].append(pt)

      #recalculate centers
      Q_new = []
      for c in assignment :
        num = [0]*len(assignment[c][0])
        denom = 0
        for x in assignment[c]:
          curr_wt = wt[tuple(x)]
          denom += curr_wt
          for m in range(len(num)):
            num[m] += x[m]*wt[tuple(x)]
        c_new = []
        for m in range(len(num)):
          c_new.append(num[m]/denom) #generalized kmeans center update : cnew = (summasion w(x)*x)/summasion w(x)
        Q_new.append(tuple(c_new))
      Q = Q_new

    return Q

def kmeans_cost(Q,dataset,wt):
  #kmeans cost for weighted dataset = summasion w(p)*d(p) ,(w(p)= wt of point p, d(p)= dist of p from its nearest center) 
  cost = 0
  dic = {}
  for c in Q:
    dic[tuple(c)] = []
  for p in dataset :
    c, dp = nearest(p,Q)
    cost += dp
    dic[tuple(c)].append(p)

  return cost,dic #dic stores key as center and value as list of points mapped to that canter

wt = {}
for pt in dataset:
  wt[tuple(pt)] = 1
# centers =  kmeans_plusplus(dataset = dataset,k = 10 )
# print(len(centers))

from sklearn.cluster import KMeans
cluster_model = KMeans(n_clusters=50, init='k-means++', random_state=0).fit(dataset)
centers = cluster_model.cluster_centers_
mod_centers = []
for j in centers:
  mod_centers.append(tuple(j))
# print(len(mod_centers))

optimal_cost,dic = kmeans_cost(mod_centers,dataset,wt)

"""Light Weight Coresets"""

def light_weight_coreset(dataset,m):
    #calculate the mean of all data points
    mu = [0]*(len(dataset[0]))
    for p in dataset :
      for k in range(len(p)):
        mu[k] += p[k]
    for k in range(len(p)):
      mu[k] = mu[k]/len(dataset)

    #first term in prob distribution
    a = 1/(2*len(dataset))

    #denominator in second term of prob distribution
    sum_dsq = 0
    mu = np.array(mu)
    for p in dataset :
      p = np.array(p)
      sum_dsq += (LA.norm(mu-p))**2

    #assign probability to each point
    q = {}
    w = []
    for p in dataset :
      p = np.array(p)
      dsq = (LA.norm(mu-p))**2
      q[tuple(p)] = a + (1/2)*(dsq/sum_dsq)
      w.append(q[tuple(p)])
    

    #sample m points from this distribution       
    a = [i for i in range(len(dataset))]
    sample = np.random.choice(a,size = m, replace = False ,p=w )

    coreset = {}
    for indx in sample:
      p = dataset[indx]
      coreset[tuple(p)] = 1/(m*q[tuple(p)]) #point and weight

    return coreset

coreset_size = [35000, 30000, 25000, 20000, 15000, 10000, 6000]
from sklearn.cluster import KMeans

for ssize in coreset_size:
  coreset = light_weight_coreset(dataset, ssize)
  """CLustering on Coreset and Comparison with optimal Kmeans solution"""
  data2 = []
  coreset_wts = []
  for p in coreset:
    data2.append(list(p))
    coreset_wts.append(coreset[p])

  # running 5 times and taking average
  avg_cost = 0
  for j in range(5):
    cluster_model = KMeans(n_clusters=50, init='k-means++', random_state=0).fit(data2, sample_weight=coreset_wts)
    centers = cluster_model.cluster_centers_
    mod_centers = []
    for j in centers:
      mod_centers.append(tuple(j))
    cost2, dic = kmeans_cost(mod_centers,dataset,coreset)
    avg_cost += cost2

  avg_cost = avg_cost/5

  print("optimal cost is --> ", optimal_cost)
  print("coreset Length", ssize)
  print("sampling cost is --> ", avg_cost)
  reduction = ((dataset.shape[0] - ssize)/dataset.shape[0])*100
  error = (abs(avg_cost - optimal_cost)/optimal_cost)*100
  print("reduction in dataset is --> ", reduction)
  print("error in clustering cost --> ", error)
